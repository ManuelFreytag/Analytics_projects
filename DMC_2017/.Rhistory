#MISSING: JOIN TO CLASS
#JOIN THE COLUMNS!!
class = subset(read.csv("01_Data/class.csv", sep = "|", header = TRUE, dec = "."), select = c(lineID, day, pid))
rm(list = ls())
library(sqldf)
order_trend_train = read.csv("02_NewFeatures/order_trend_train.csv")
#MISSING: JOIN TO CLASS
order_trends = sqldf("select distinct pid, day, decomp_cyclic_order_norm, decomp_trend_order_norm")
#JOIN THE COLUMNS!!
class = subset(read.csv("01_Data/class.csv", sep = "|", header = TRUE, dec = "."), select = c(lineID, day, pid))
#Calculate the start periode of the class set
reqStart = max(train$day)%%7
perZero = min(class$day)
#recalculate day for the joining possibility
class["day"] = class$day - perZero + reqStart
class_export = sqldf("select class.lineID, order_trends.decomp_cyclic_order_norm, order_trends.decomp_trend_order_norm from class left join order_trends on (class.pid = order_trends.pid and class.day = order_trends.day)")
rm(list = ls())
library(sqldf)
order_trend_train = read.csv("02_NewFeatures/order_trend_train.csv")
train = subset(read.csv("01_Data/train.csv"), select = c(lineID, day, pid))
#MISSING: JOIN TO CLASS
order_trends = sqldf("select distinct train.pid, train.day, decomp_cyclic_order_norm, decomp_trend_order_norm from order_trend_train join train on order_trend_train.lineID = train.lineID")
#JOIN THE COLUMNS!!
class = subset(read.csv("01_Data/class.csv", sep = "|", header = TRUE, dec = "."), select = c(lineID, day, pid))
#Calculate the start periode of the class set
reqStart = max(train$day)%%7
perZero = min(class$day)
#recalculate day for the joining possibility
class["day"] = class$day - perZero + reqStart
class_export = sqldf("select class.lineID, order_trends.decomp_cyclic_order_norm, order_trends.decomp_trend_order_norm from class left join order_trends on (class.pid = order_trends.pid and class.day = order_trends.day)")
rm(list = ls())
library(sqldf)
order_trend_train = read.csv("02_NewFeatures/order_trend_train.csv")
train = subset(read.csv("01_Data/train.csv", sep = "|"), select = c(lineID, day, pid))
#MISSING: JOIN TO CLASS
order_trends = sqldf("select distinct train.pid, train.day, decomp_cyclic_order_norm, decomp_trend_order_norm from order_trend_train join train on order_trend_train.lineID = train.lineID")
class = subset(read.csv("01_Data/class.csv", sep = "|", header = TRUE, dec = "."), select = c(lineID, day, pid))
#Calculate the start periode of the class set
reqStart = max(train$day)%%7
perZero = min(class$day)
max(train$day)
#recalculate day for the joining possibility
class["day"] = class$day - perZero + reqStart + 1
class_export = sqldf("select class.lineID, order_trends.decomp_cyclic_order_norm, order_trends.decomp_trend_order_norm from class left join order_trends on (class.pid = order_trends.pid and class.day = order_trends.day)")
summary(class_export)
rm(list = ls())
library(sqldf)
library(plyr)
options(scipen=999)
#The working directory should be changed of course
train = read.csv("01_Data/train.csv", sep = "|", header = TRUE, dec = ".")
train = rename(train, c("order" = "label"))
#TREND ANALYSIS
#1) Filter price, pid, day
trend = sqldf("select lineID, pid, label, day from train")
#2) generate list of all pids, ordered by occurance
ord_pids = sqldf("select pid from (select count(pid) as cpid, pid
from trend group by pid) as t1 order by t1.cpid desc")
#Definition of a short function for trend decomposition into a data.frame
decomposition = function(x, y, pid, freq = 7){
ts_tmp = ts(y, frequency = freq)
decompose_price = stl(ts_tmp, "periodic")
#Cyclic component
app = data.frame(day = x)
app["pid"] = pid
app["cyclic"] = as.numeric(decompose_price$time.series[,1])
#initializing it with trend 0
trend_series = c(0)
for(j in c(2:max_day)){
trend_series = c(trend_series, (decompose_price$time.series[j,2]-decompose_price$time.series[j-1,2]))
}
app["trend"] = mean(trend_series)
return(app)
}
#OPERATIVE PART OF THE TREND ANALYISIS
#Defining important vectors and data.frames to be filled
trend_order_vars = c()
trend_order_perc = c()
max_day = max(trend$day)
day_df = sqldf("select day from trend group by day")
decomposition_order_df = data.frame(day = integer(), pid = character(), cyclic = double(), trend = double())
counter = 0
#Starting the training process
for(i in ord_pids$pid){
counter = counter + 1
print(counter)
#Select next pid
tmp = trend[trend$pid == i,]
#3) Average the order values of each product sold on each day
trend_tmp = sqldf("select day_df.day, t1.avg_label
from day_df left join (select day, CAST(avg(label) as float) as avg_label from tmp group by day) as t1 on t1.day = day_df.day")
#GENERAL TREND ANALYSIS
#Check if the label is changing, else skip it
if(sum(is.na(trend_tmp$avg_label)) == 0){
#if(length(table(trend_tmp$)) > 1){}
#Calculate the general trend variable for order
app = decomposition(trend_tmp$day, trend_tmp$avg_label, i)
#Append to final df
decomposition_order_df = rbind(decomposition_order_df, app)
}
fm = lm(trend_tmp$avg_label ~ trend_tmp$day)
trend_order_vars = c(trend_order_vars, fm$coefficients[2])
trend_order_perc = c(trend_order_perc, fm$coefficients[2]/fm$coefficients[1])
}
pids = data.frame(pid = unique(decomposition_order_df["pid"]), trend_order = trend_order_vars, trend_order_perc = trend_order_perc)
#JOINING RESULTS INTO A DF
#Average the results
final_trend = sqldf("select trend.lineID, decomposition_order_df.pid, decomposition_order_df.day, decomposition_order_df.cyclic as decomp_cyclic_order_norm, decomposition_order_df.trend as decomp_trend_order_norm
from trend join decomposition_order_df on (decomposition_order_df.pid = trend.pid and decomposition_order_df.day = trend.day)")
#Join the averaged results to it.
final_trend = sqldf("select final_trend.*, pids.trend_order, pids.trend_order_perc from final_trend join pids on final_trend.pid = pids.pid")
train_export = subset(sqldf("select train.lineID, final_trend.*
from train left join final_trend on train.lineID = final_trend.lineID"), select = -c(2,3,4))
train_export[is.na(train_export)] = 0
write.csv(round(train_export, digits = 5), "order_trend_train.csv", row.names = FALSE)
write.csv(decomposition_order_df, "decomp_order.csv")
summary(decomposition_order_df)
#JOIN TO THE CLASS SET
class = subset(read.csv("01_Data/class.csv", sep = "|", header = TRUE, dec = "."), select = c(lineID, day, pid))
reqStart = max(train$day)%%7
perZero = min(class$day)
#recalculate day for the joining possibility (which is day 2 at min)
class["day"] = class$day - perZero + reqStart + 1
class_export = sqldf("select class.lineID, cyclic as decomp_cyclic_order_norm, trend as decomp_trend_order_norm from class left join decomposition_order_df on (class.pid = decomposition_order_df.pid and class.day = decomposition_order_df.day)")
write.csv(round(class_export, digits), "order_trend_class.csv", row.names = FALSE)
summary(class_export)
class['day']
summary(class['day'])
class_export[is.na(class_export)] = 0
write.csv(round(class_export, digits = 5), "order_trend_class.csv", row.names = FALSE)
trend = subset(read.csv("train_ABC_Trend_AVGActions.csv"), select = c(lineID,decomp_cyclic_counts_norm,decomp_trend_counts_norm,decomp_cyclic_price_norm,decomp_trend_price_norm,mean_click,mean_basket,mean_order))
clustering_similar = subset(read.csv("train_clustering_revenue_sum.csv"), select = c(lineID_Clean,sum_rev,actions_pid,sum_rev_man,actions_man,price_excess_comp))
names(clustering_similar)[names(clustering_similar)=="lineID_Clear"] <- "lineID"
order_trend = read.csv("order_trend_train.csv")
trend = merge(trend, clustering_similar, by = "lineID")
trend = merge(trend, order_trend, by = "lineID")
names(clustering_similar)[names(clustering_similar)=="lineID_Clear"] = "lineID"
trend = merge(trend, clustering_similar, by = "lineID")
names(clustering_similar)[names(clustering_similar)=="lineID_Clear"] = "lineID"
names(clustering_similar)[names(clustering_similar)=="lineID_Clean"] = "lineID"
trend = merge(trend, clustering_similar, by = "lineID")
trend = subset(read.csv("02_NewFeatures/train_ABC_Trend_AVGActions.csv"), select = c(lineID,decomp_cyclic_counts_norm,decomp_trend_counts_norm,decomp_cyclic_price_norm,decomp_trend_price_norm,mean_click,mean_basket,mean_order))
trend = merge(trend, clustering_similar, by = "lineID")
summary(trend)
trend = merge(trend, cluster_ident, by = "lineID")
cluster_ident = read.csv("train_cluster_single_prod.csv")
summary(trend)
trend = merge(trend, cluster_ident, by = "lineID")
names(cluster_ident)[names(cluster_ident)=="lineID_Clean"] = "lineID"
trend = merge(trend, cluster_ident, by = "lineID")
summary(trend)
cluster_ident = subset(read.csv("train_cluster_single_prod.csv"), select = -c(X))
names(cluster_ident)[names(cluster_ident)=="lineID_Clean"] = "lineID"
trend$X = NULL
summary(trend)
write.csv(trend, as "trainManu.csv")
write.csv(trend, "trainManu.csv")
trend = subset(read.csv("02_NewFeatures/class_ABC_Trend_AVGActions.csv"), select = c(lineID,decomp_cyclic_counts_norm,decomp_trend_counts_norm,decomp_cyclic_price_norm,decomp_trend_price_norm,mean_click,mean_basket,mean_order))
clustering_similar = subset(read.csv("class_clustering_revenue_sum.csv"), select = c(lineID_Clean,sum_rev,actions_pid,sum_rev_man,actions_man,price_excess_comp))
names(clustering_similar)[names(clustering_similar)=="lineID_Clean"] = "lineID"
order_trend = read.csv("order_trend_class.csv")
cluster_ident = subset(read.csv("class_cluster_single_prod.csv"), select = -c(X))
names(cluster_ident)[names(cluster_ident)=="lineID_Clean"] = "lineID"
trend = merge(trend, clustering_similar, by = "lineID")
trend = merge(trend, order_trend, by = "lineID")
trend = merge(trend, cluster_ident, by = "lineID")
write.csv(trend, "classManu.csv")
summary(trend)
trend = subset(read.csv("02_NewFeatures/class_ABC_Trend_AVGActions.csv"), select = c(lineID,decomp_cyclic_counts_norm,decomp_trend_counts_norm,decomp_cyclic_price_norm,decomp_trend_price_norm,mean_click,mean_basket,mean_order))
clustering_similar = subset(read.csv("class_clustering_revenue_sum.csv"), select = c(lineID_Clean,sum_rev,actions_pid,sum_rev_man,actions_man,price_excess_comp))
class_final = merge(trend, clustering_similar, by = "lineID")
names(clustering_similar)[names(clustering_similar)=="lineID_Clean"] = "lineID"
names(cluster_ident)[names(cluster_ident)=="lineID_Clean"] = "lineID"
class_final = merge(trend, clustering_similar, by = "lineID")
class_final = merge(class_final, order_trend, by = "lineID")
class_final = merge(trend, clustering_similar, by = "lineID")
class_final = merge(class_final, order_trend, by = "lineID")
class_final = merge(trend, clustering_similar, by = "lineID")
class_final = merge(class_final, order_trend, by = "lineID")
class_final = merge(trend, clustering_similar, by = "lineID")
class_final = merge(class_final, cluster_ident, by = "lineID")
order_trend = read.csv("order_trend_class.csv")
order_trend["lineID"] = cluster_ident["lineID"]
class_final = merge(class_final, order_trend, by = "lineID")
summary(class_final)
trend = subset(read.csv("02_NewFeatures/class_ABC_Trend_AVGActions.csv"), select = c(lineID,decomp_cyclic_counts_norm,decomp_trend_counts_norm,decomp_cyclic_price_norm,decomp_trend_price_norm,mean_click,mean_basket,mean_order))
clustering_similar = subset(read.csv("class_clustering_revenue_sum.csv"), select = c(lineID_Clean,sum_rev,actions_pid,sum_rev_man,actions_man,price_excess_comp))
names(clustering_similar)[names(clustering_similar)=="lineID_Clean"] = "lineID"
order_trend = read.csv("order_trend_class.csv")
cluster_ident = subset(read.csv("class_cluster_single_prod.csv"), select = -c(X))
names(cluster_ident)[names(cluster_ident)=="lineID_Clean"] = "lineID"
class_final = merge(trend, clustering_similar, by = "lineID")
class_final = merge(class_final, order_trend, by = "lineID")
class_final = merge(class_final, cluster_ident, by = "lineID")
write.csv(class_final, "classManu.csv")
summary(class_final)
trend = subset(read.csv("02_NewFeatures/train_ABC_Trend_AVGActions.csv"), select = c(lineID,decomp_cyclic_counts_norm,decomp_trend_counts_norm,decomp_cyclic_price_norm,decomp_trend_price_norm,mean_click,mean_basket,mean_order))
trend = subset(read.csv("02_NewFeatures/class_ABC_Trend_AVGActions.csv"), select = c(lineID,decomp_cyclic_counts_norm,decomp_trend_counts_norm,decomp_cyclic_price_norm,decomp_trend_price_norm,mean_click,mean_basket,mean_order))
clustering_similar = subset(read.csv("class_clustering_revenue_sum.csv"), select = c(lineID_Clean,sum_rev,actions_pid,sum_rev_man,actions_man,price_excess_comp))
names(clustering_similar)[names(clustering_similar)=="lineID_Clean"] = "lineID"
order_trend = read.csv("order_trend_class.csv")
trend = subset(read.csv("02_NewFeatures/class_ABC_Trend_AVGActions.csv"), select = c(lineID,decomp_cyclic_counts_norm,decomp_trend_counts_norm,decomp_cyclic_price_norm,decomp_trend_price_norm,mean_click,mean_basket,mean_order))
clustering_similar = subset(read.csv("class_clustering_revenue_sum.csv"), select = c(lineID_Clean,sum_rev,actions_pid,sum_rev_man,actions_man,price_excess_comp))
names(clustering_similar)[names(clustering_similar)=="lineID_Clean"] = "lineID"
order_trend = read.csv("order_trend_class.csv")
order_trend["lineID"] = clustering_similar$lineID
cluster_ident = subset(read.csv("class_cluster_single_prod.csv"), select = -c(X))
names(cluster_ident)[names(cluster_ident)=="lineID_Clean"] = "lineID"
class_final = merge(trend, clustering_similar, by = "lineID")
class_final = merge(class_final, order_trend, by = "lineID")
class_final = merge(class_final, cluster_ident, by = "lineID")
write.csv(class_final, "classManu.csv")
trend = subset(read.csv("02_NewFeatures/train_ABC_Trend_AVGActions.csv"), select = c(lineID,decomp_cyclic_counts_norm,decomp_trend_counts_norm,decomp_cyclic_price_norm,decomp_trend_price_norm,mean_click,mean_basket,mean_order))
clustering_similar = subset(read.csv("train_clustering_revenue_sum.csv"), select = c(lineID_Clean,sum_rev,actions_pid,sum_rev_man,actions_man,price_excess_comp))
names(clustering_similar)[names(clustering_similar)=="lineID_Clean"] = "lineID"
order_trend = read.csv("order_trend_train.csv")
cluster_ident = subset(read.csv("train_cluster_single_prod.csv"), select = -c(X))
names(cluster_ident)[names(cluster_ident)=="lineID_Clean"] = "lineID"
trend = merge(trend, clustering_similar, by = "lineID")
trend = merge(trend, order_trend, by = "lineID")
trend = merge(trend, cluster_ident, by = "lineID")
write.csv(trend, "trainManu.csv")
hist(trend$price_excess_comp)
#NAIVE SOLUTIONS
setwd("C:/Users/Manuel/OneDrive/Project_Datascience/R/DMC_2017_Seminar/01_Data")
all.train = subset(read.csv("train.csv", sep = "|"), select = c(pid, revenue))
train.train = head(all.train, (3/4)*length(all.train[[1]]))
test.train = tail(all.train, (1/4)*length(all.train[[1]]))
rmse_own = function(actual, predicted){
error <- actual - predicted
rmse = sqrt(mean(error^2))
return(rmse)
}
all.train = subset(read.csv("orgin/train.csv", sep = "|"), select = c(pid, revenue))
all.train = subset(read.csv("origin/train.csv", sep = "|"), select = c(pid, revenue))
train.train = head(all.train, (3/4)*length(all.train[[1]]))
test.train = tail(all.train, (1/4)*length(all.train[[1]]))
table(train.train)
table(train.train$revenue)
mfv = table(train.train$revenue)[[1]]
mfv
mfv = name(table(train.train$revenue)[[1]])
mfv = rownames(table(train.train$revenue)[[1]])
mfv
test = table(train.train$revenue)
name(test[[1]])
id(test[[1]])
colnames((test))
colnames(test)
rownames(test)
rownames(test)[[1]]
mfv = rownames(table(train.train$revenue))[[1]]
ns1 = rmse_own(test.train$revenue, rep(mfv, 3))
ns1 = rmse_own(test.train$revenue, rep(as.numeric(mfv), 3))
ns1 = rmse_own(test.train$revenue, rep(as.numeric(mfv), length(test.train[[1]])))
ns2 = rmse_own(test.train$revenue, rep(avg, length(test.train[[1]])))
avg = mean(train.train$revenue)
ns2 = rmse_own(test.train$revenue, rep(avg, length(test.train[[1]])))
?aggregate
test = aggregate(train.train, by = c(pid), FUN = "average")
test = aggregate(train.train, by = c(pid), FUN = "mean")
test = aggregate(train.train, by = c("pid"), FUN = "mean")
test = aggregate(train.train, by = list("pid"), FUN = "mean")
test = aggregate(train.train, by = list(train.train$pid), FUN = "mean")
names(test)[names(test)=="revenue"] = "avg_revenue"
avg = merge(train.train, test, by = "pid")
ns3 = rmse_own(avg$revenue, avg$avg_revenue)
sample = read.csv2("joined_train_org_sample.csv")
sample = read.csv2("origin/joined_train_org_sample.csv")
avg_sample = merge(sample, test, by = "pid")
ns3s = rmse_own(sample$revenue, avg_sample)
ns3s = rmse_own(sample$revenue, as.numeric(avg_sample))
ns3s = rmse_own(sample$revenue, as.numeric(avg_sample$avg_revenue))
ns3s = rmse_own(sample$revenue, avg_sample$avg_revenue)
ns1s = rmse_own(sample$revenue, rep(mfv, length(sample[[1]])))
ns1s = rmse_own(sample$revenue, rep(mfv, as.numeric(length(sample[[1]]))))
ns1s = rmse_own(sample$revenue, rep(as.numeric(mfv),length(sample[[1]])))
ns2s = rmse_own(sample$revenue, rep(avg, length(sample[[1]])))
ns2s = rmse_own(sample$revenue, rep(avg$revenue, length(sample[[1]])))
ns2s = rmse_own(sample$revenue, rep(avg, length(sample[[1]])))
#NAIVE SOLUTIONS
setwd("C:/Users/Manuel/OneDrive/Project_Datascience/R/DMC_2017_Seminar/01_Data")
all.train = subset(read.csv("origin/train.csv", sep = "|"), select = c(pid, revenue))
#NAIVE SOLUTIONS
setwd("C:/Users/Manuel/OneDrive/Project_Datascience/R/DMC_2017_Seminar/01_Data")
all.train = subset(read.csv("origin/train.csv", sep = "|"), select = c(pid, revenue))
sample = subset(read.csv("origin/sample"))
train.train = head(all.train, (3/4)*length(all.train[[1]]))
test.train = tail(all.train, (1/4)*length(all.train[[1]]))
rmse_own = function(actual, predicted){
error <- actual - predicted
rmse = sqrt(mean(error^2))
return(rmse)
}
#1) Impute with the most frequent value
#select the most frequent value
mfv = rownames(table(train.train$revenue))[[1]]
ns1 = rmse_own(test.train$revenue, rep(as.numeric(mfv), length(test.train[[1]])))
#2) Impute with the average
avg = mean(train.train$revenue)
ns2 = rmse_own(test.train$revenue, rep(avg, length(test.train[[1]])))
#3) Impute with the average of each pid
test = aggregate(train.train, by = list(train.train$pid), FUN = "mean")
names(test)[names(test)=="revenue"] = "avg_revenue"
avg_all = merge(train.train, test, by = "pid")
ns3 = rmse_own(avg_all$revenue, avg$avg_revenue)
#Benchmarks for our sample
sample = read.csv2("origin/joined_train_org_sample.csv")
ns1s = rmse_own(sample$revenue, rep(as.numeric(mfv),length(sample[[1]])))
ns2s = rmse_own(sample$revenue, rep(avg, length(sample[[1]])))
avg_sample = merge(sample, test, by = "pid")
ns3s = rmse_own(sample$revenue, avg_sample$avg_revenue)
ns3 = rmse_own(avg_all$revenue, avg_all$avg_revenue)
#NAIVE SOLUTIONS
setwd("C:/Users/Manuel/OneDrive/Project_Datascience/R/DMC_2017_Seminar/01_Data")
all.train = subset(read.csv("origin/train.csv", sep = "|"), select = c(pid, revenue))
train.train = head(all.train, (3/4)*length(all.train[[1]]))
test.train = tail(all.train, (1/4)*length(all.train[[1]]))
rmse_own = function(actual, predicted){
error <- actual - predicted
rmse = sqrt(mean(error^2))
return(rmse)
}
#1) Impute with the most frequent value
#select the most frequent value
mfv = rownames(table(train.train$revenue))[[1]]
ns1 = rmse_own(test.train$revenue, rep(as.numeric(mfv), length(test.train[[1]])))
#2) Impute with the average
avg = mean(train.train$revenue)
ns2 = rmse_own(test.train$revenue, rep(avg, length(test.train[[1]])))
#3) Impute with the average of each pid
test = sqldf("select avg(train.revenue) as avg_revenue from train.train group by pid")
avg_all = merge(train.train, test, by = "pid")
ns3 = rmse_own(avg_all$revenue, avg_all$avg_revenue)
#Benchmarks for our sample
sample = read.csv2("origin/joined_train_org_sample.csv")
ns1s = rmse_own(sample$revenue, rep(as.numeric(mfv),length(sample[[1]])))
ns2s = rmse_own(sample$revenue, rep(avg, length(sample[[1]])))
avg_sample = merge(sample, test, by = "pid")
ns3s = rmse_own(sample$revenue, avg_sample$avg_revenue)
#NAIVE SOLUTIONS
setwd("C:/Users/Manuel/OneDrive/Project_Datascience/R/DMC_2017_Seminar/01_Data")
all.train = subset(read.csv("origin/train.csv", sep = "|"), select = c(pid, revenue))
train.train = head(all.train, (3/4)*length(all.train[[1]]))
test.train = tail(all.train, (1/4)*length(all.train[[1]]))
rmse_own = function(actual, predicted){
error <- actual - predicted
rmse = sqrt(mean(error^2))
return(rmse)
}
#1) Impute with the most frequent value
#select the most frequent value
mfv = rownames(table(train.train$revenue))[[1]]
ns1 = rmse_own(test.train$revenue, rep(as.numeric(mfv), length(test.train[[1]])))
#2) Impute with the average
avg = mean(train.train$revenue)
ns2 = rmse_own(test.train$revenue, rep(avg, length(test.train[[1]])))
#3) Impute with the average of each pid
test = sqldf("select avg(train.revenue) as avg_revenue from train.train group by pid")
avg_all = merge(train.train, test, by = "pid", all.x = TRUE)
ns3 = rmse_own(avg_all$revenue, avg_all$avg_revenue)
#Benchmarks for our sample
sample = read.csv2("origin/joined_train_org_sample.csv")
ns1s = rmse_own(sample$revenue, rep(as.numeric(mfv),length(sample[[1]])))
ns2s = rmse_own(sample$revenue, rep(avg, length(sample[[1]])))
avg_sample = merge(sample, test, by = "pid", all.x = TRUE)
ns3s = rmse_own(sample$revenue, avg_sample$avg_revenue)
#NAIVE SOLUTIONS
setwd("C:/Users/Manuel/OneDrive/Project_Datascience/R/DMC_2017_Seminar/01_Data")
all.train = subset(read.csv("origin/train.csv", sep = "|"), select = c(pid, revenue))
train.train = head(all.train, (3/4)*length(all.train[[1]]))
test.train = tail(all.train, (1/4)*length(all.train[[1]]))
rmse_own = function(actual, predicted){
error <- actual - predicted
rmse = sqrt(mean(error^2))
return(rmse)
}
#1) Impute with the most frequent value
#select the most frequent value
mfv = rownames(table(train.train$revenue))[[1]]
ns1 = rmse_own(test.train$revenue, rep(as.numeric(mfv), length(test.train[[1]])))
#2) Impute with the average
avg = mean(train.train$revenue)
ns2 = rmse_own(test.train$revenue, rep(avg, length(test.train[[1]])))
#3) Impute with the average of each pid
library(sqldf)
test = sqldf("select avg(train.revenue) as avg_revenue from train.train group by pid")
avg_all = merge(train.train, test, by = "pid", all.x = TRUE)
ns3 = rmse_own(avg_all$revenue, avg_all$avg_revenue)
#Benchmarks for our sample
sample = read.csv2("origin/joined_train_org_sample.csv")
ns1s = rmse_own(sample$revenue, rep(as.numeric(mfv),length(sample[[1]])))
ns2s = rmse_own(sample$revenue, rep(avg, length(sample[[1]])))
avg_sample = merge(sample, test, by = "pid", all.x = TRUE)
ns3s = rmse_own(sample$revenue, avg_sample$avg_revenue)
#NAIVE SOLUTIONS
setwd("C:/Users/Manuel/OneDrive/Project_Datascience/R/DMC_2017_Seminar/01_Data")
all.train = subset(read.csv("origin/train.csv", sep = "|"), select = c(pid, revenue))
train_train = head(all.train, (3/4)*length(all.train[[1]]))
test.train = tail(all.train, (1/4)*length(all.train[[1]]))
rmse_own = function(actual, predicted){
error <- actual - predicted
rmse = sqrt(mean(error^2))
return(rmse)
}
#1) Impute with the most frequent value
#select the most frequent value
mfv = rownames(table(train.train$revenue))[[1]]
ns1 = rmse_own(test.train$revenue, rep(as.numeric(mfv), length(test.train[[1]])))
#2) Impute with the average
avg = mean(train.train$revenue)
ns2 = rmse_own(test.train$revenue, rep(avg, length(test.train[[1]])))
#3) Impute with the average of each pid
library(sqldf)
test = sqldf("select avg(train.revenue) as avg_revenue from train_train group by pid")
avg_all = merge(train.train, test, by = "pid", all.x = TRUE)
ns3 = rmse_own(avg_all$revenue, avg_all$avg_revenue)
#Benchmarks for our sample
sample = read.csv2("origin/joined_train_org_sample.csv")
ns1s = rmse_own(sample$revenue, rep(as.numeric(mfv),length(sample[[1]])))
ns2s = rmse_own(sample$revenue, rep(avg, length(sample[[1]])))
avg_sample = merge(sample, test, by = "pid", all.x = TRUE)
ns3s = rmse_own(sample$revenue, avg_sample$avg_revenue)
#NAIVE SOLUTIONS
setwd("C:/Users/Manuel/OneDrive/Project_Datascience/R/DMC_2017_Seminar/01_Data")
all.train = subset(read.csv("origin/train.csv", sep = "|"), select = c(pid, revenue))
train_train = head(all.train, (3/4)*length(all.train[[1]]))
test.train = tail(all.train, (1/4)*length(all.train[[1]]))
rmse_own = function(actual, predicted){
error <- actual - predicted
rmse = sqrt(mean(error^2))
return(rmse)
}
#1) Impute with the most frequent value
#select the most frequent value
mfv = rownames(table(train.train$revenue))[[1]]
ns1 = rmse_own(test.train$revenue, rep(as.numeric(mfv), length(test.train[[1]])))
#2) Impute with the average
avg = mean(train.train$revenue)
ns2 = rmse_own(test.train$revenue, rep(avg, length(test.train[[1]])))
#3) Impute with the average of each pid
library(sqldf)
test = sqldf("select avg(train_train.revenue) as avg_revenue from train_train group by pid")
avg_all = merge(train.train, test, by = "pid", all.x = TRUE)
ns3 = rmse_own(avg_all$revenue, avg_all$avg_revenue)
#Benchmarks for our sample
sample = read.csv2("origin/joined_train_org_sample.csv")
ns1s = rmse_own(sample$revenue, rep(as.numeric(mfv),length(sample[[1]])))
ns2s = rmse_own(sample$revenue, rep(avg, length(sample[[1]])))
avg_sample = merge(sample, test, by = "pid", all.x = TRUE)
ns3s = rmse_own(sample$revenue, avg_sample$avg_revenue)
View(test)
#NAIVE SOLUTIONS
setwd("C:/Users/Manuel/OneDrive/Project_Datascience/R/DMC_2017_Seminar/01_Data")
all.train = subset(read.csv("origin/train.csv", sep = "|"), select = c(pid, revenue))
train_train = head(all.train, (3/4)*length(all.train[[1]]))
test.train = tail(all.train, (1/4)*length(all.train[[1]]))
rmse_own = function(actual, predicted){
error <- actual - predicted
rmse = sqrt(mean(error^2))
return(rmse)
}
#1) Impute with the most frequent value
#select the most frequent value
mfv = rownames(table(train.train$revenue))[[1]]
ns1 = rmse_own(test.train$revenue, rep(as.numeric(mfv), length(test.train[[1]])))
#2) Impute with the average
avg = mean(train.train$revenue)
ns2 = rmse_own(test.train$revenue, rep(avg, length(test.train[[1]])))
#3) Impute with the average of each pid
library(sqldf)
test = sqldf("select pid, avg(train_train.revenue) as avg_revenue from train_train group by pid")
avg_all = merge(train.train, test, by = "pid", all.x = TRUE)
ns3 = rmse_own(avg_all$revenue, avg_all$avg_revenue)
#Benchmarks for our sample
sample = read.csv2("origin/joined_train_org_sample.csv")
ns1s = rmse_own(sample$revenue, rep(as.numeric(mfv),length(sample[[1]])))
ns2s = rmse_own(sample$revenue, rep(avg, length(sample[[1]])))
avg_sample = merge(sample, test, by = "pid", all.x = TRUE)
ns3s = rmse_own(sample$revenue, avg_sample$avg_revenue)
ns3s
#NAIVE SOLUTIONS
setwd("C:/Users/Manuel/OneDrive/Project_Datascience/R/DMC_2017_Seminar/01_Data")
all.train = subset(read.csv("origin/train.csv", sep = "|"), select = c(pid, revenue))
train_train = head(all.train, (3/4)*length(all.train[[1]]))
test.train = tail(all.train, (1/4)*length(all.train[[1]]))
rmse_own = function(actual, predicted){
error <- actual - predicted
rmse = sqrt(mean(error^2))
return(rmse)
}
#1) Impute with the most frequent value
#select the most frequent value
mfv = rownames(table(train.train$revenue))[[1]]
ns1 = rmse_own(test.train$revenue, rep(as.numeric(mfv), length(test.train[[1]])))
#2) Impute with the average
avg = mean(train.train$revenue)
ns2 = rmse_own(test.train$revenue, rep(avg, length(test.train[[1]])))
#3) Impute with the average of each pid
library(sqldf)
test = sqldf("select pid, avg(train_train.revenue) as avg_revenue from train_train group by pid")
avg_all = merge(train.train, test, by = "pid", all.x = TRUE)
ns3 = rmse_own(avg_all$revenue, avg_all$avg_revenue)
#Benchmarks for our sample
sample = read.csv2("origin/joined_train_org_sample.csv")
ns1s = rmse_own(sample$revenue, rep(as.numeric(mfv),length(sample[[1]])))
ns2s = rmse_own(sample$revenue, rep(avg, length(sample[[1]])))
avg_sample = merge(sample, test, by = "pid", all.x = TRUE)
avg_sample(is.na(avg_sample)) = 0
ns3s = rmse_own(sample$revenue, avg_sample$avg_revenue)
avg_sample[is.na(avg_sample)] = 0
ns3s = rmse_own(sample$revenue, avg_sample$avg_revenue)
